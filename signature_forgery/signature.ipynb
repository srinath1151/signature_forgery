{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e169e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08583b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d705b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: code to mount to drive\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128\n",
    "# Define the Embedding Model (Feature Extractor)\n",
    "def build_embedding_model(input_shape):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                64, (3, 3), activation=\"relu\", input_shape=input_shape\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D(4, 4),\n",
    "            tf.keras.layers.Conv2D(512, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D(4, 4),\n",
    "            tf.keras.layers.Flatten(),\n",
    "        ],\n",
    "        name=\"embedding_model\",\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inputs for Siamese Network\n",
    "input_a = tf.keras.layers.Input(shape=(size, size, 1), name=\"input1\")\n",
    "input_b = tf.keras.layers.Input(shape=(size, size, 1), name=\"input2\")\n",
    "\n",
    "# Shared Embedding Model\n",
    "embedding_model = build_embedding_model((size, size, 1))\n",
    "em_one = embedding_model(input_a)\n",
    "em_two = embedding_model(input_b)\n",
    "\n",
    "# Use Absolute Difference Instead of Concatenation\n",
    "# Corrected the line below to apply the Lambda function correctly\n",
    "\n",
    "\n",
    "def abs_diff(tensors):\n",
    "    return tf.abs(tensors[0] - tensors[1])\n",
    "\n",
    "\n",
    "out = tf.keras.layers.Lambda(\n",
    "    abs_diff, output_shape=lambda input_shapes: input_shapes[0], name=\"abs_diff\"\n",
    ")([em_one, em_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d320d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Layers for Classification\n",
    "out = tf.keras.layers.Dense(64, activation=\"relu\")(out)\n",
    "out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"Output\")(out)\n",
    "# Create and Compile Model\n",
    "model = tf.keras.models.Model([input_a, input_b], out)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc66a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "data_dir = \"\\signatures\"\n",
    "genuine_dir = os.path.join(data_dir, \"full_org\")\n",
    "forgery_dir = os.path.join(data_dir, \"full_forg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32777c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\n"
     ]
    }
   ],
   "source": [
    "def load_signature_images(genuine_path, forgery_path, target_size=(128, 128)):\n",
    "    def load_images(path):\n",
    "        images = []\n",
    "        for image_file in os.listdir(path):\n",
    "            img = cv2.imread(os.path.join(path, image_file), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                images.append(cv2.resize(img, target_size))\n",
    "        return np.array(images)\n",
    "\n",
    "    return load_images(genuine_path), load_images(forgery_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_images, forgery_images = load_signature_images(genuine_dir, forgery_dir)\n",
    "genuine_images, forgery_images = (\n",
    "    genuine_images / 255.0,\n",
    "    forgery_images / 255.0,\n",
    ")  # Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4fc2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pairs for Siamese Training\n",
    "def create_pairs(genuine, forged):\n",
    "    pairs, labels = [], []\n",
    "    for i in range(min(len(genuine), len(forged))):\n",
    "        pairs.append([genuine[i], genuine[(i + 1) % len(genuine)]])  # Genuine pair\n",
    "        labels.append(1)\n",
    "        pairs.append([genuine[i], forged[i]])  # Forged pair\n",
    "        labels.append(0)\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "pairs, labels = create_pairs(genuine_images, forgery_images)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pairs, labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5661dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model - No wandb callbacks at all\n",
    "history = model.fit(\n",
    "    [X_train[:, 0], X_train[:, 1]],\n",
    "    y_train,\n",
    "    validation_data=([X_test[:, 0], X_test[:, 1]], y_test),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=0.0001),\n",
    "    ],\n",
    ")\n",
    "# Evaluate Model\n",
    "loss, accuracy = model.evaluate([X_test[:, 0], X_test[:, 1]], y_test, verbose=0)\n",
    "predictions = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeaf0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, predicted_labels))\n",
    "# Create the directory if it doesn't exist\n",
    "model_save_dir = \"/content/drive/MyDrive/signature_verification_model(3)\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Save the model to Google Drive\n",
    "model_save_path = os.path.join(model_save_dir, \"siamese_signature_model.keras\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "wandb.init(project=\"signature-verification\")\n",
    "# Log results to wandb AFTER training is complete\n",
    "# This avoids any issues with wandb trying to monitor the model during training\n",
    "wandb.config.update(\n",
    "    {\n",
    "        \"model_type\": \"siamese_network\",\n",
    "        \"image_size\": 128,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"epochs\": 20,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log test metrics\n",
    "wandb.log(\n",
    "    {\n",
    "        \"Test Accuracy\": accuracy,\n",
    "        \"Test Loss\": loss,\n",
    "    }\n",
    ")\n",
    "# Log training history\n",
    "for epoch in range(len(history.history[\"loss\"])):\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training Loss\": history.history[\"loss\"][epoch],\n",
    "            \"Validation Loss\": history.history[\"val_loss\"][epoch],\n",
    "            \"Training Accuracy\": history.history[\"accuracy\"][epoch],\n",
    "            \"Validation Accuracy\": history.history[\"val_accuracy\"][epoch],\n",
    "        }\n",
    "    )\n",
    "# Log the Model to wandb as an Artifact\n",
    "artifact = wandb.Artifact(\"siamese_signature_model\", type=\"model\")\n",
    "artifact.add_file(model_path)\n",
    "wandb.log_artifact(artifact)\n",
    "# Save the embedding model separately for later feature extraction\n",
    "embedding_model_path = \"signature_embedding_model.keras\"\n",
    "embedding_model.save(embedding_model_path)\n",
    "embed_artifact = wandb.Artifact(\"signature_embedding_model\", type=\"model\")\n",
    "embed_artifact.add_file(embedding_model_path)\n",
    "wandb.log_artifact(embed_artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of sample pairs and predictions\n",
    "def visualize_pairs(X_pairs, y_true, y_pred, num_pairs=5):\n",
    "    fig, axes = plt.subplots(num_pairs, 3, figsize=(12, 3 * num_pairs))\n",
    "\n",
    "    # Get some random samples\n",
    "    indices = np.random.choice(len(y_true), num_pairs, replace=False)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get the pair of images\n",
    "        img1, img2 = X_pairs[idx, 0], X_pairs[idx, 1]\n",
    "        true_label = \"Genuine\" if y_true[idx] == 1 else \"Forgery\"\n",
    "        pred_label = \"Genuine\" if y_pred[idx] == 1 else \"Forgery\"\n",
    "        match = \"✓\" if y_true[idx] == y_pred[idx] else \"✗\"\n",
    "\n",
    "        # Plot the images\n",
    "        axes[i, 0].imshow(img1, cmap=\"gray\")\n",
    "        axes[i, 0].set_title(\"Reference Signature\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(img2, cmap=\"gray\")\n",
    "        axes[i, 1].set_title(\"Test Signature\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        # Add a text annotation for the prediction\n",
    "        axes[i, 2].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            f\"True: {true_label}\\nPred: {pred_label}\\n{match}\",\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\" if y_true[idx] == y_pred[idx] else \"red\",\n",
    "        )\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Create and log the visualization\n",
    "vis_fig = visualize_pairs(X_test, y_test, predicted_labels)\n",
    "wandb.log({\"Sample Predictions\": wandb.Image(vis_fig)})\n",
    "plt.close(vis_fig)\n",
    "\n",
    "\n",
    "# Function to create a signature verification tool\n",
    "def create_signature_verifier(embedding_model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Creates a function that can verify if two signatures match\n",
    "\n",
    "    Args:\n",
    "        embedding_model: The trained embedding model\n",
    "        threshold: Similarity threshold (higher = more strict)\n",
    "\n",
    "    Returns:\n",
    "        A function that takes two signature images and returns True if they match\n",
    "    \"\"\"\n",
    "\n",
    "    def verify_signature(reference_sig, test_sig):\n",
    "        # Preprocess images\n",
    "        ref = cv2.resize(reference_sig, (size, size)) / 255.0\n",
    "        test = cv2.resize(test_sig, (size, size)) / 255.0\n",
    "\n",
    "        # Reshape for model input\n",
    "        ref = ref.reshape(1, size, size, 1)\n",
    "        test = test.reshape(1, size, size, 1)\n",
    "\n",
    "        # Get embeddings\n",
    "        ref_embedding = embedding_model.predict(ref)\n",
    "        test_embedding = embedding_model.predict(test)\n",
    "        # Calculate similarity (can use cosine similarity or Euclidean distance)\n",
    "        similarity = 1 - np.sum(np.abs(ref_embedding - test_embedding)) / np.sum(\n",
    "            np.abs(ref_embedding) + np.abs(test_embedding)\n",
    "        )\n",
    "\n",
    "        return similarity > threshold, similarity\n",
    "\n",
    "    return verify_signature\n",
    "\n",
    "\n",
    "# Log completion and finish wandb run\n",
    "wandb.log({\"status\": \"completed\"})\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Training and evaluation completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
